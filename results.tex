In this section, we empirically evaluate Sparta's performance in a series of experiments on six benchmarks, ranging from image recognition, natural language processing to random forest and time series prediction, which are implemented based on Tensorflow and executed through Sparta's actuator interface. We first overview the machine learning benchmarks that we consider and our experimental methodology. We then present our results.

\subsection{Machine Learning Benchmarks}

To comprehensively evaluate the efficacy and efficiency of Sparta, we implemented 6 machine learning benchmarks, which consist of four categories: image recognition, natural language processing, ensemble learning and time series analysis. We aim to test Sparta on a variety of machine learning applications that represent different execution patterns.

\subsubsection{WTB\_Train}

is an image recognition application that we use as a benchmark to train a convolutional neural network (CNN)~\cite{ref:cnn} based on ResNet50~\cite{ref:resnet}. The training dataset contains animal images from a wildlife monitoring system called "Where's The Bear" (WTB)~\cite{ref:wtb}. "Where's The Bear" is an end-to-end distributed data acquisition and analytical system that automatically analyzes camera trap images collected by cameras sited at the Sedgwick Natural Reserve~\cite{ref:sedgwick} in Santa Barbara County, California. In total, there are five classes that we consider: Bear, Coyote, Deer, Bird and Empty, by which we label images for training tasks. We also up-sampled minority classes using the Keras Image Data Generator~\cite{ref:datagen}, since the class size is unbalanced due to the frequency of animal occurrences. Doing so ensures that the classification model is not biased. We resized every image in the dataset to $1920 \times 1080$, and for each class, the dataset contains 60 images used to train the CNN model. Once the training is complete, the application stores this model in hdf5 format in object storage. 

The WTB\_Train application has a cold start at the beginning of the execution, since it loads a pre-trained neural network model and a large dataset. Once it completes loading, the entire training process has relatively consistent CPU usage and temperature. 

\subsubsection{WTB\_Inf}

inferences the type of wildlife in camera trap pictures based on the model trained by WTB\_Train. It loads the trained hdf5 model at the beginning and, for each picture, it assigns probabilities to five classes we consider in the training dataset by Softmax function. In each experiment, we assign 20 pictures for WTB\_Inf to inference. In terms of the execution pattern, WTB\_Inf runs in short bursts as opposed to WTB\_Train. Therefore, the CPU usage and temperature fluctuate dramatically throughout the execution of this benchmark.

\subsubsection{MNIST}

 is a dataset containing grayscale pictures of handwritten digits, in which it has 60,000 examples as training set and 10,000 examples as testing set. Based on the dataset, we train a 2-layer convolutional neural network~\cite{ref:MNIST} and test its accuracy in the third application. In contrast to WTB benchmarks, the size of pictures is smaller ($28 \times 28$) and the model is simplified in MNIST. 

\subsubsection{BiLSTM} is a sentiment analysis application based on a dataset of the Internet Movie Database (IMDB) movie reviews. It consists of 25,000 sequences each for training and testing. The model is constructed as a bidirectional LSTM with a classification layer using sigmoid activation function. We train the model by the training dataset and validate its performance in classifying sentiment by testing dataset. Since it has a large dataset and a complex model, the execution pattern is long-running and consistent in CPU usage and temperature.


\subsubsection{Decision\_Forest} is an implementation of deep neural decision forests~\cite{ref:decision_forest} that classifies high-earning individuals from the pool. The benchmark leverages the United States Census Income Dataset~\cite{ref:uci} that has 48,843 instances with 14 features, including age, education, occupation, etc. The dataset is split up that the training part has 32,561 instances and the testing part has 16,282 instances. The application has three phases: it firstly processes the dataset by encoding input features. Then, it trains a deep neural decision tree model. Based on that, the application trains a neural decision forest model consists of a set of neural decision trees. Therefore, the usage and temperature of CPU increasingly grows throughout the process.


\subsubsection{Time\_Series} is a time series prediction application built on the climate data recorded by the Max Planck Institute for Biogeochemistry~\cite{ref:jena}. The dataset has 14 features such as temperature, pressure, humidity, etc. and the sampling frequency is 10 minutes. The time frame of the dataset ranges from Jan. 10th, 2009 to Dec. 31st, 2016. The application uses 300,693 rows to train a single-layer LSTM model, by which we can predict temperature in next 72 timestamps (12 hours) given the temperature in the past 720 timestamps (120 hours). By this benchmark, we intend to evaluate Sparta on an application with a lightweight model and a large dataset.

% Execution pattern table

\subsection{Experimental Setup}

Each edge cloud node used in the experiments is an Intel NUC~\cite{ref:nuc} (6i7KYK) with two Intel Core i7-6770HQ 4-core processors (6M Cache, 2.60 GHz) and 32GB of DDR4-2133+ RAM connected via two channels. We use dynamic voltage and frequency scaling (DVFS) to control the frequency of CPU from 0.8GHz to 3.5GHz.

To simulate the natural temperature in Sedgwick natural reserve, we create three thermal environments in an isolated cooler that represent cold, neutral and hot ambient temperature. In the cold scenario, the ambient temperature is 2.6\degree C and the CPU of NUC runs under 40\degree C in idle status. In the neutral scenario, the CPU of NUC starts at 51\degree C under the ambient temperature of 23.9\degree C. The hot scenario increases the ambient and CPU temperature to 43.8\degree C and 68\degree C respectively.

\begin{figure}
\includegraphics[width=\textwidth]{figures/thermal.jpg}
\caption{Three thermal environments in the experiment} \label{cold}
\end{figure}

There are two goals of the Sparta scheduler: the first is to limit the CPU temperature under the threshold; the second is to accelerate tasks without overheating the edge cloud. To evaluate these two objectives, we execute 6 machine learning benchmarks under 3 modes of Sparta scheduler. In each experiment, Sparta takes inputs of task program, corresponding workload dataset and a threshold temperature. To keep the comparison consistent across 3 thermal environments, we use 75\degree C as the threshold temperature for all experiments.

In 3 modes of Sparta, we execute each machine learning benchmark repeatedly 100 times under 3 thermal environments (totally $3 \times 3 \times 6 \times 100 = 5400$ executions) and report relevant metrics, both mean and standard deviation, to compare the efficacy and efficiency among Annealing, AIMD and Hybrid modes.


\subsection{Application Efficacy}

We first measure the stabilization time for six benchmarks. We define stabilization time as the elapsed time from the start to the point all CPU temperatures in the sampling time window are within $[T_s - T_d, T_s]$, where $T_s$ is the threshold and $T_d$ is a slack variable (3 \degree C by default). During each of the 10 consecutive executions (1 epoch) of benchmarks, we record the duration when Sparta scheduler stabilizes the CPU temperature according to the threshold. As showed in the first part of Table \ref{tab:stab}, we report the mean and stdev of stabilization time for each benchmark in 3 modes. Hybrid mode uses less time to stabilize CPU temperature than Annealing and AIMD in all six benchmarks. It performs even better in WTB\_Inf benchmark that has short burst execution pattern and volatile CPU temperature.

%Stabilization table

\begin{table}[t]
\caption{The mean and stdev of \textbf{stabilization time} in seconds for 6 machine learning benchmarks in 3 Sparta modes. Compared to Annealing and AIMD, Hybrid mode uses less time to stabilize CPU temperature across all benchmarks and all thermal scenarios. }\label{tab:stab}
\vspace{1mm}
\centering
\resizebox{350pt}{!}{\input{tables/stab}}
\newline
\vspace{3mm}
\newline
\resizebox{300pt}{!}{\input{tables/stab_th}}
\end{table}


%Runtime table


As the second part of Table \ref{tab:stab} presenting the result in the thermal dimension, Hybrid mode also uses less time to stabilize CPU temperature across all 3 thermal environments, comparing to Annealing and AIMD. Averagely, Hybrid mode uses 43.61 seconds in stabilization phase, in contrast to 61.16 seconds in Annealing and 65.9 seconds in AIMD. Hybrid mode's performance is even better in hot scenario, which is the key use case for edge devices to prevent overheating in Sedgwick natural reserve.

\begin{table}[t]
\caption{The mean and stdev of \textbf{execution time} in seconds for 6 machine learning benchmarks in 3 modes of Sparta. Compared to Annealing and AIMD, Hybrid mode uses less time to complete tasks across all benchmarks and all thermal scenarios. }\label{tab:runtime}
\vspace{1mm}
\centering
\resizebox{350pt}{!}{\input{tables/runtime}}
\newline
\vspace{3mm}
\newline
\resizebox{300pt}{!}{\input{tables/runtime_th}}
\end{table}


We next empirically evaluate the execution time of six benchmarks by Sparta scheduler. In the first part of Table \ref{tab:runtime}, we report the mean and stdev of execution time for each benchmark under 3 modes. In average, the Hybrid mode completes the task of each benchmarks faster than Annealing and AIMD. Given the stdev and degree of freedom, we also run student t-test among 3 modes for each benchmark and confirm that the execution time by Hybrid is smaller than Annealing and AIMD with a statistic significance level of 5\%. Table \ref{tab:runtime} also indicates the speedup of Hybrid over Annealing and AIMD, ranging from 1.04x to 1.32x. 

The second part of Table \ref{tab:runtime} demonstrates the average execution time in 3 thermal scenarios. On average, Hybrid mode completes the task in 126.61 seconds, in comparison with 146.09 seconds by Annealing and 147.46 seconds by AIMD. Hybrid mode provides 1.16x and 1.14x speedup respectively over Annealing and AIMD. These results show that Sparta in Hybrid mode efficiently executes more workloads than Annealing and AIMD mode under the same temperature threshold. 


%RSME of all temp table

\begin{table}[t]
\caption{The mean and stdev of \textbf{RMSE} of all temperature samples for 6 benchmarks in 3 modes of Sparta. Compared to Annealing and AIMD, Hybrid mode has less RSME to threshold temperature across all benchmarks and all thermal scenarios.}\label{tab:rsme}
\vspace{1mm}
\centering
\resizebox{350pt}{!}{\input{tables/rsme}}
\newline
\vspace{3mm}
\newline
\resizebox{300pt}{!}{\input{tables/rsme_th}}
\end{table}


To investigate the error from the sampling temperature and threshold, we next evaluate the Root Mean Square Error (RMSE) of all temperature samples in the executions. We define $RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(T_i - \hat{T})^2}$, where $T_i$ is a sample of CPU temperature, $\hat{T}$ is the temperature threshold and n is the number of temperature samples. In Table \ref{tab:rsme}, we display the mean and stdev of RMSE of all CPU temperature samples. The RMSE of Hybrid mode is the least across all six benchmarks among other two modes. Hybrid mode also has the lowest RMSE in all three thermal scenarios. In average, Hybrid has 6.34 as RMSE for all temperature samples from threshold. 

%Percentage of temp below threshold table

\begin{table}[t]
\caption{The mean and stdev of \textbf{PTBT} (Percentage of Temperature Below Threshold) for 6 benchmarks in 3 modes of Sparta. Due to their inherent algorithm, Annealing has the lowest PTBT value and AIMD has the highest, whereas the Hybrid mode has the PTBT value in-between across all benchmarks and all thermal scenarios.}\label{tab:percent}
\vspace{1mm}
\centering
\resizebox{350pt}{!}{\input{tables/percent}}
\newline
\vspace{3mm}
\newline
\resizebox{300pt}{!}{\input{tables/percent_th}}
\end{table}


Lastly, we report the percentage of samples below threshold temperature in six benchmarks. The first part of Table \ref{tab:percent} manifests the mean and stdev of PTBT (Percentage of Temperature Below Threshold) for six benchmarks. Because Annealing mode uses a probabilistic algorithm, it results in the lowest PTBT metric among three modes. Since AIMD mode multiplicatively decreases the CPU frequency whenever a temperature over threshold is detected, it has the highest PTBT metrics in all six benchmarks. Combined with Annealing and AIMD, the PTBT of Hybrid mode is between the other two modes. This relationship holds for all three thermal scenarios, as depicted in the second part of Table \ref{tab:percent}. Hybrid mode maintains 94.4\% of all temperature samples below the threshold. Thus, we consider the above results strong proofs of Sparta's efficacy in preventing overheating of edge devices and executing a variety of tasks as efficiently as possible.





